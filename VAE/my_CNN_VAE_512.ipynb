{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras          import backend as K\n",
    "\n",
    "from keras.layers   import Input, Lambda, Dense\n",
    "from keras.models   import Model, load_model, model_from_json\n",
    "from keras.datasets import mnist\n",
    "from keras.losses   import binary_crossentropy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test  = x_test.astype('float32')  / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_autoencoder = load_model('mnist_autoencoder.h5',compile=False)\n",
    "mnist_autoencoder = load_model('mnist_dAE.h5')\n",
    "\n",
    "mnist_autoencoder = model_from_json(mnist_autoencoder.to_json())\n",
    "\n",
    "vae_latent_dim = 2\n",
    "\n",
    "mnist_encoder = mnist_autoencoder.layers[1]\n",
    "mnist_decoder = mnist_autoencoder.layers[2]\n",
    "\n",
    "# mnist_latent_dim = mnist_encoder.outputs[0].shape[1].value\n",
    "mnist_latent_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_loss(x_raw, x_decoded):\n",
    "    xen_loss = K.mean(binary_crossentropy(x_raw, x_decoded) * 784)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xen_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mnist_encoder.layers.pop()\n",
    "\n",
    "vae_inputs = mnist_encoder(mnist_autoencoder.input)\n",
    "vae_inputs = Dense(mnist_latent_dim, name='latent_AE')(vae_inputs)\n",
    "\n",
    "z_mean     = Dense(vae_latent_dim, name='z_mean'   )(vae_inputs)\n",
    "z_log_var  = Dense(vae_latent_dim, name='z_log_var')(vae_inputs)\n",
    "z          = Lambda(sampling, output_shape=(vae_latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "vae_encoder = Model(mnist_autoencoder.input, [z_mean, z_log_var, z], name='vae_encoder')\n",
    "\n",
    "\n",
    "vae_latent = Input(shape=(vae_latent_dim,), name='z_sampling')\n",
    "vae_latent_output = Dense(mnist_latent_dim, name='latent_sampling', activation='relu')(vae_latent)\n",
    "\n",
    "mnist_decoder._layers[0].batch_input_shape = (None, mnist_latent_dim)\n",
    "mnist_decoder = model_from_json(mnist_decoder.to_json())\n",
    "\n",
    "vae_decoder = Model(vae_latent, mnist_decoder(vae_latent_output), name='vae_decoder')\n",
    "\n",
    "\n",
    "vae_input  = mnist_autoencoder.input\n",
    "vae_output = vae_decoder( vae_encoder(vae_input)[2] )\n",
    "\n",
    "vae = Model(\n",
    "    inputs  = vae_input, \n",
    "    outputs = vae_output, \n",
    "    name    = 'MNIST_VAE'\n",
    ")\n",
    "\n",
    "vae.compile(optimizer='adam',loss=vae_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      " - 42s - loss: 188.1337 - val_loss: 164.6922\n",
      "Epoch 2/200\n",
      " - 40s - loss: 165.3721 - val_loss: 160.3439\n",
      "Epoch 3/200\n",
      " - 41s - loss: 162.3167 - val_loss: 158.3324\n",
      "Epoch 4/200\n",
      " - 40s - loss: 160.6279 - val_loss: 156.3414\n",
      "Epoch 5/200\n",
      " - 40s - loss: 159.5121 - val_loss: 155.7881\n",
      "Epoch 6/200\n",
      " - 40s - loss: 158.6423 - val_loss: 154.6933\n",
      "Epoch 7/200\n",
      " - 40s - loss: 158.0387 - val_loss: 153.9828\n",
      "Epoch 8/200\n",
      " - 40s - loss: 157.4495 - val_loss: 154.1500\n",
      "Epoch 9/200\n",
      " - 40s - loss: 157.1259 - val_loss: 153.2956\n",
      "Epoch 10/200\n",
      " - 40s - loss: 156.7341 - val_loss: 153.0341\n",
      "Epoch 11/200\n",
      " - 40s - loss: 156.3688 - val_loss: 152.3349\n",
      "Epoch 12/200\n",
      " - 40s - loss: 156.2077 - val_loss: 151.7815\n",
      "Epoch 13/200\n",
      " - 41s - loss: 155.8835 - val_loss: 151.5178\n",
      "Epoch 14/200\n",
      " - 40s - loss: 155.6944 - val_loss: 152.2620\n",
      "Epoch 15/200\n",
      " - 40s - loss: 155.4831 - val_loss: 151.2476\n",
      "Epoch 16/200\n",
      " - 41s - loss: 155.2635 - val_loss: 151.9040\n",
      "Epoch 17/200\n",
      " - 40s - loss: 155.1810 - val_loss: 150.6316\n",
      "Epoch 18/200\n",
      " - 40s - loss: 154.9278 - val_loss: 150.9350\n",
      "Epoch 19/200\n",
      " - 40s - loss: 154.8369 - val_loss: 150.7785\n",
      "Epoch 20/200\n",
      " - 40s - loss: 154.6731 - val_loss: 150.3975\n",
      "Epoch 21/200\n",
      " - 40s - loss: 154.4953 - val_loss: 150.9953\n",
      "Epoch 22/200\n",
      " - 40s - loss: 154.4233 - val_loss: 151.6886\n",
      "Epoch 23/200\n",
      " - 40s - loss: 154.2345 - val_loss: 150.1215\n",
      "Epoch 24/200\n",
      " - 40s - loss: 154.1943 - val_loss: 150.1775\n",
      "Epoch 25/200\n",
      " - 40s - loss: 153.9536 - val_loss: 150.7501\n",
      "Epoch 26/200\n",
      " - 40s - loss: 153.8601 - val_loss: 150.2943\n",
      "Epoch 27/200\n",
      " - 40s - loss: 153.9090 - val_loss: 149.3501\n",
      "Epoch 28/200\n",
      " - 40s - loss: 153.7973 - val_loss: 149.7608\n",
      "Epoch 29/200\n",
      " - 40s - loss: 153.7276 - val_loss: 149.9906\n",
      "Epoch 30/200\n",
      " - 40s - loss: 153.5675 - val_loss: 149.6964\n",
      "Epoch 31/200\n",
      " - 40s - loss: 153.6356 - val_loss: 149.8419\n",
      "Epoch 32/200\n",
      " - 40s - loss: 153.6565 - val_loss: 149.9032\n",
      "Epoch 33/200\n",
      " - 40s - loss: 153.3448 - val_loss: 149.6344\n",
      "Epoch 34/200\n",
      " - 40s - loss: 153.4327 - val_loss: 149.2796\n",
      "Epoch 35/200\n",
      " - 40s - loss: 153.3625 - val_loss: 149.1724\n",
      "Epoch 36/200\n",
      " - 40s - loss: 153.2644 - val_loss: 149.7915\n",
      "Epoch 37/200\n",
      " - 40s - loss: 153.1708 - val_loss: 149.3188\n",
      "Epoch 38/200\n",
      " - 40s - loss: 153.3351 - val_loss: 149.8243\n",
      "Epoch 39/200\n",
      " - 40s - loss: 153.1294 - val_loss: 149.0288\n",
      "Epoch 40/200\n",
      " - 40s - loss: 153.1440 - val_loss: 148.8595\n",
      "Epoch 41/200\n",
      " - 40s - loss: 153.0818 - val_loss: 149.3504\n",
      "Epoch 42/200\n",
      " - 40s - loss: 152.9961 - val_loss: 149.0415\n",
      "Epoch 43/200\n",
      " - 40s - loss: 152.9944 - val_loss: 149.3633\n",
      "Epoch 44/200\n",
      " - 40s - loss: 152.8251 - val_loss: 149.1509\n",
      "Epoch 45/200\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(\n",
    "    np.expand_dims(x_train, axis=3),\n",
    "    np.expand_dims(x_train, axis=3),\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_data=(np.expand_dims(x_test, axis=3), np.expand_dims(x_test, axis=3)),\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = vae_encoder.predict(np.expand_dims(x_test, axis=3), batch_size=128)[0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, alpha=0.7, s=5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]) * 1\n",
    "        x_decoded = vae_decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
